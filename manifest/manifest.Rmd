---
title: "Total Survey Error Framework R Ecosystem"
author: "Jorge Cimentada"
date: "`r Sys.Date()`"
output: rmarkdown::html_document
---

As an attempt to organize my ideas on the measurement error packages that we're building, I'm writing this document that will outline all of the ideas around the package. This
document will not touch upon the technicalities of the survey error framework given that this is already quite advanced both in the literature and in the software
implementation. However, this document will describe how we can build an R package that merges both worlds starting from a top-to-bottom description of several R packages.

## Overview of weighting and modeling

The final objective of the total survey error packages is to have a default and reference package for all things related to measurement error/weighting and statistical
modeling. Currently, defining a liner model in R is quite simple (in pseudocode):

```{r eval = FALSE}
lm(poltrst ~ ppltrst + stflife + stfeco, data = ess_data)
```

This function call requires only two arguments: the formula definition and the data which contains the columns. Additionally, it allows to specify a simple weight column.  In a
more complicated framework, `cite:oberski_complex` introduced the package `lavaan.survey` which allows to run Structural Equational Modeling with complex survey designs using the
`survey` package. Using this framework, the pseudocode from above becomes:

```{r eval = FALSE}
# Load packages and data
library(lavaan.survey)
# devtools::install_github("ropensci/essurvey")
library(essurvey)

set_email("cimentadaj@gmail.com")

# Download data
ess_spain <- import_country("Spain", 4)

# 1) Define model and run it
spain_model <- "ppltrst ~ stflife + trstprl + stfeco"
lavaan_fit <- sem(spain_model, data=spain)

# 2) Define survey design
survey_design <-
  svydesign(
    ids = ~ psu + idno,
    strata = ~ stratify,
    weights = ~ prob,
    data = ess_spain
  )

options(survey.lonely.psu = "adjust")

# 3) Adjust your model for a complex survey design
survey_fit <- lavaan.survey(lavaan_fit, survey_design)
```

There are now three steps in the pseudo code: 1) define and run the model, 2) create the survey design and 3) adjust your initial model for the complex survey design.
Note that even though we're using a different package for Structural Equational Modeling, the estimation from above simply ran a linear model just as `lm` would do.
The added benefit is that we can account for the complex survey structure of the data generating process (this specific model can also be run with the `survey`
package but only for generalized linear models and not for more complex models such as structural ones. Moreover, the survey package does not
allow to add the correlation between variables in the function call, a key step that allows measurement error to be corrected).

If we integrated the above into a function called `lm_tse` (`tse` for total survey error), we could directly simplify the code above and end up with a shorter and concise function call (in pseudocode):

```{r eval = FALSE}
# Define survey design
survey_design <-
  svydesign(
    ids = ~ psu + idno,
    strata = ~ stratify,
    weights = ~ prob,
    data = ess_spain
  )

lm_tse(
  formula = ppltrst ~ stflife + trstprl + stfeco,
  data = ess_spain,
  complex_wt = survey_design
)
```

This expression is more intuitive as it only has two steps: 1) define complex survey design and 2) run model. It is assumed that inside `lm_tse` the same proceedure as above will take place, allowing the wrapper `lm_tse` not to worry about argument checking or the correspondance between the complex design and the data. This is already taken care of by the `lavaan.survey` function.

## A framework for incorporating measurement error

An ideal measurement error workflow would take the above and add only one step: define a measurement error design. Just as you take care in designing your complex survey structure, measurement error should also require a thorough yet simple definition of the relationship between variables. Below I extend the last example to include an abstract design strategy of measurement error:

```{r eval = FALSE}
# 1) Define survey design
survey_design <-
  svydesign(
    ids = ~ psu + idno,
    strata = ~ stratify,
    weights = ~ prob,
    data = ess_spain
  )

# 2) Define measurement error design

# 2.1) Variable relationship
# Each row shows variables which share a common method
variable_relationships <-
  "~ stflife + stfeco
   ~ ppltrst + trstprl"

# 2.2) Get measurement error data (could be SQP or your own data)
me_data <- get_estimates()

# 2.3) Define your measurement error design
me_design <-
  medesign(
    model = variable_relationships,
    data = ess_data,
    me_data = me_data
  )

# 3) Run model
lm_tse(
  formula = ppltrst ~ stflife + trstprl + stfeco,
  data = ess_data,
  complex_wt = survey_design,
  me = me_design
)

```

Let's define each step at a time.

* Step 1: classic way of defining your complex survey design. Many references on how to do this such as [this](http://r-survey.r-forge.r-project.org/survey/) and [this](http://asdfree.com/).

* Step 2.1: Define the relationship between your variables. Here the user might specify which variables share a common method, which variables are standardized, which variables are sumscores,
among other things... This 'model definition' will borrow many  ideas from the `lavaan` package in terms of parsing the model from a string. In any case, the syntax of this model definition should be very flexible and have syntax declarations for each operation described above (share common method, standardized variables, etc...)

* Step 2.2: Obtain measurement error estimates. This could be through SQP or your own set of the quality estimates.

* Step 2.3: Combine the model definition, the data that will be used in the analysis and the measurement error estimates. This step takes care of making sure that all variables defined in the model are indeed in the measurement error data frame as well as in the data. Moreover, it checks whether the variables defined have actual values in the measurement error dataframe.

* Step 3: Define the measurement error model, with two additional arguments: `complex_wt` and `me`, which were explained above.

This last step is familiar yet adds the new `me` argument. How would this work out with the previous `lavaan.survey` expression? Below the curtain, `lavaan.survey` and `measurementfree` can be integrated:

```{r eval = FALSE}
library(lavaan.survey)
library(essurvey)
library(magrittr)
library(sqpr)
library(measurementfree)

# Choose your selected variables
selected_vars <- c("ppltrst",
                   "trstprl",
                   "stflife",
                   "stfeco")

set_email("cimentadaj@gmail.com")
# Download the ESS data and clear missing values
ess4es_complete <- import_country("Spain", 4)[c("idno", selected_vars)]
ess4es <- ess4es_complete[complete.cases(ess4es_complete[, -1]), selected_vars]

# Download SQP data
sqp_login("asqme", "asqme")
quality <-
  get_sqp(
    study = "ESS Round 4",
    question_name = selected_vars,
    country = "ES",
    lang = "spa"
  )

# Force variables in the same order 
quality <- quality[match(selected_vars, quality$question), ]

# Correlation matrix with quality in diagonal:
corrected_corr <- me_correlate(x = ess4es, diag_adj = quality$quality)

# Subtract the cmv from the observed correlation
corrected_corr <-
  corrected_corr %>% 
  me_cmv_cor(me_data = quality, stfeco, stflife) %>%
  me_cmv_cor(me_data = quality, ppltrst, trstprl)

corrected_corr <- cov2cor(as.matrix(corrected_corr[, -1]))
model <- "ppltrst ~ stflife + trstprl + stfeco"

# Run linear model based on corrected correlation matrix 
fit_corrected <-
  sem(model,
      sample.cov = corrected_corr,
      sample.nobs = nrow(ess4es)
      )

# Merge SDDF weight data with ESS data
weight_vars <- c("idno", "psu", "stratify", "prob")
weights_spain <- import_sddf_country("Spain", 4)[weight_vars]
svy_ess4es <- merge(ess4es_complete, weights_spain, by = "idno")

# Define complex survey design
survey_design <-
  svydesign(
    ids = ~ psu + idno,
    strata = ~ stratify,
    weights = ~ prob,
    data = svy_ess4es
  )

# Model based on corrected correlation matrix but weighted
options(survey.lonely.psu = "adjust")
fit_corrected_svy <- lavaan.survey(fit_corrected, survey_design)
```

However, the above strategy is also very tedious. The purpose of the pseudo code `lm_tse` is to streamline all of this automatically with the help of the `me_design` function. `me_design` will be largely responsible for defining the measurement error relationship between variables and checking that they are available for estimation:

```{r, eval = FALSE}
variable_relationships <-
  "~ stflife + stfeco
   ~ ppltrst + trstprl"
n
medesign(
  model = variable_relationships,
  data = ess_data,
  me_data = me_data
)
```


`medesign` takes care of doing two things. First, it should parse the variable relationship object to check that:

1) The variables defined in the model are present in `me_data`.
2) The variables defined in the model have no missing values in `me_data`.
3) The variables defined in the model are present in `data`.
4) The variables defined in the model are not complete missing in `data`.

This function will lazily evaluate the arguments and delay the computation to another function which is in charge of executing the 'plan'. However, `medesign` should be responsible for parsing the model definition in detail. For that, we need to set some rules on how is something defined.

We need to be able to define:

1) A sumscore (define with a `=`) 
2) A standardized sumscore (defined with a `std()`) 
3) When observed variables share a common method (defined as `~`) 
4) When observed variable shares common method with a sumscore (defined similarly as `~`) 
5) When observed variable shares common method with a standardized sumscore (defined similarly as `~`) 
6) When a sum score shares common method with a sum score (defined similarly as `~`)
7) When a sum score shares common method with a standardized sumscore (defined similarly as `~`) 
8) When a standardized sumscore shares common method with a standardized sumscore (defined similarly as `~`)

Below is an attempt to generate the syntax that defines a model. All of the code below is pseudo code, so variable names are made up.

```{r }
model_definition <-
  "#1)
   sumscore = var1 + var2

   #2)
   std_sumscore1 = std(var3 + var4)
   std_sumscore2 = std(var5 + var6)

   #3)
   ~ var7 + var8

   #4)
   ~ var9 + sumscore

   #5)
   ~ var10 + std_sumscore1

   #6)
   ~ sumscore + std_sumscore1

   #7)
   ~ std_sumscore1 + std_sumscore2"
```

Each step in `model_definition` exemplifies each one of the described list of properties above. 
Anything defined as `some_var = std(another_var + another_var1)` will be treated as the creation of a variable with a special operator `std` to define standardized variables and anything
defined with `~` will be treated as sharing a common method. Internally, `me_design` will know which variables are standardized by flagging the variable.

The parsing of this model syntax will be used similarly to `lavaan:::lavParseModelString`.

# Advantages

The strategy described above would have finally 3 steps that are transparent to the user:

```{r eval = FALSE}
ess_spain <- # get your initial data

  # 1) Define survey design
  survey_design <-
    svydesign(
      ids = ~ psu + idno,
      strata = ~ stratify,
      weights = ~ prob,
      data = ess_spain
    )

# 2) Define measurement error design

# 2.1) Variable relationship
# Each row shows variables which share a common method
variable_relationships <-
  "~ stflife + stfeco
   ~ ppltrst + trstprl"

# 2.2) Get measurement error data (could be SQP or your own data)
me_data <- get_estimates()

# 2.3) Define your measurement error design
me_design <-
  medesign(
    model = variable_relationships,
    data = ess_data,
    me_data = me_data
  )

# 3) Run model
tse_result <-
  lm_tse(
    formula = ppltrst ~ stflife + trstprl + stfeco,
    data = ess_data,
    complex_wt = survey_design,
    me = me_design
  )
```

These are easy to understand steps and make the process linear when definition the Total Survey Error framework. The advantages of this process is that most of the estimation happening here is done through `lavaan` which is a package very mature. This means that all processes will revert back to the argument checking, estimation strategies and errors raised by the package. Moreover, the result will always be of class `lavaan`, which means that it can be used with **any** other function from the lavaan software (`summary`, `coef`, `parameterEstimates`, etc..) as well as other packages (`broom`).

# Syntax parsing

```{r}

model.syntax <-
  "#1)
   sumscore = var1 + var2

   #2)
   std_sumscore1 = std(I(var3*w1 + w4 + sin(20)) + I(var4*w2) + I(var5*w3))
   # std_sumscore2 = std(var5 + var6)

   #3)
   ~ var7 + var8

   #4)
   ~ var9 + sumscore

   #5)
   ~ var10 + std_sumscore1

   #6)
   ~ sumscore + std_sumscore1

   #7)
   ~ std_sumscore1 + std_sumscore2"

operators <- c("~", "=")

me_parse_model <- function(model_syntax = " ",
                               as_list = FALSE,
                               warn = TRUE,
                               debug_code = FALSE) {

  if (length(model_syntax) == 0) {
    stop("Empty measurement error model_syntax")
  }

  # Delete comments
  model_syntax <- gsub("[#!].*(?=\n)", "", model_syntax, perl = TRUE)
  # Delete lines separated by ;
  model_syntax <- gsub(";", "\n", model_syntax, fixed = TRUE)
  # Delete tabs spaces
  model_syntax <- gsub("[ \t]+", "", model_syntax, perl = TRUE)
  # Delete new spaces of more than 2 to only one
  model_syntax <- gsub("\n{2,}", "\n", model_syntax, perl = TRUE)
  # separate intro vector
  model <- unlist(strsplit(model_syntax, "\n"))

  # Figure out lines which have operators to exclude other lines
  start.idx <-
    grep(
      paste0("[", paste0(operators, collapse = ""), "]", collapse = ""),
      model
    )

  if (length(start.idx) == 0L) {
    stop("Model does not contain measurement error syntax (operators must be one of ", paste0("'", operators, "'", collapse = ", "), ")") #nolintr
  }
  
  if (start.idx[1] > 1L) {
    for (el in 1:(start.idx[1] - 1L)) {
      if (nchar(model[el]) > 0L) {
        warning("No operator found in this syntax line: ",
                model[el], "\n",
                "This syntax line will be ignored!")
      }
    }
  }

  end.idx <- c(start.idx[-1] - 1, length(model))
  model.orig <- model
  model <- character(length(start.idx))

  for (i in 1:length(start.idx)) {
    model[i] <- paste(model.orig[start.idx[i]:end.idx[i]], collapse = "")
  }

  idx.wrong <-
    which(
      !grepl(paste0("[", paste0(operators, collapse = ""), "]"), model)
    )

  if (length(idx.wrong) > 0) {
    cat("Missing operator in formula(s):\n")
    print(model[idx.wrong])
    stop("Syntax error in measurement error model_syntax")
  }

  idx.wrong <- which(grepl("^\\+", model))
  if (length(idx.wrong) > 0) {
    cat("Some formula(s) start with a plus (+) sign:\n")
    print(model[idx.wrong])
    stop("Syntax error in measurement error model_syntax")
  }

  ind_formula <- grep("^~", model)
  model[ind_formula] <- paste0("X", seq_along(ind_formula), model[ind_formula])
  
  FLAT.lhs <- character(0)
  FLAT.op <- character(0)
  FLAT.rhs <- character(0)
  FLAT.rhs.mod.idx <- integer(0)
  FLAT.std <- logical(0)
  FLAT.idx <- 0L
  MOD.idx <- 0L

  for (i in 1:length(model)) {
    x <- model[i]
    if (debug_code) {
      cat("Formula to parse:\n")
      print(x)
      cat("\n")
    }
    
    if (grepl(operators[1], x, fixed = TRUE)) {
      op <- operators[1]
    } else if (grepl(operators[2], x, fixed = TRUE)) {
      op <- operators[2]
    } else {
      stop("Unknown operator in ", model[i], ". Operators must be one of ", paste0("'", operators, "'", collapse = ", "), ".") #nolintr
    }

    # Identify where the operator is
    op.idx <- regexpr(op, x)

    # Separate string
    lhs <- substr(x, 1L, op.idx - 1L)
    rhs <- substr(x, op.idx + attr(op.idx, "match.length"), nchar(x))

    if (substr(rhs, 1, 1) == "+") {
      rhs <- substr(rhs, 2, nchar(rhs))
    }

    LHS <- strsplit(lhs, split = "+", fixed = TRUE)[[1]]
    LHS <- gsub("^\\S*\\*", "", LHS)

    if (!all(make.names(LHS) == LHS)) {
      stop("Left hand side (lhs) of this formula:\n    ", 
           lhs,
           " ",
           op,
           " ",
           rhs,
           "\n    contains a reserved word (in R): ", 
           dQuote(LHS[!make.names(LHS) == LHS]),
           "\n    see ?reserved for a list of reserved words in R", 
           "\n    please use a variable name that is not a reserved word in R"
           )
    }

    rhs_empty <- op == "~" & (rhs == "" | rhs == "1")
    if (rhs_empty) {
      stop("Right hand side (rhs) of this formula:\n  ", 
           lhs,
           " ",
           op,
           " ",
           rhs,
           "\n  is empty"
           )
    }

    lhs.names <- lhs

    rhs <- gsub("\\(?([-]?[0-9]*\\.?[0-9]*)\\)?\\?", "start(\\1)\\*", rhs)

    # If it's standardized, replace it because syntax_parse_rhs
    # fails with std() and save that it's std for later
    std_sscore <- grepl("^std\\(.+\\)$", rhs)
    if (std_sscore) {
      rhs <- gsub("^std\\(|\\)$", "", rhs)

      # Used below to detect the formula as std sscore
      is_std <- TRUE
    } else {
      is_std <- FALSE
    }

    out <- syntax_parse_rhs(rhs = as.formula(paste0("~", rhs)))
    
    if (debug_code) print(out)
    
    for (l in 1:length(lhs.names)) {
      for (j in 1:length(out)) {
        rhs.name <- names(out)[j]
        
        FLAT.idx <- FLAT.idx + 1L
        FLAT.lhs[FLAT.idx] <- lhs.names[l]
        FLAT.op[FLAT.idx] <- op
        FLAT.rhs[FLAT.idx] <- rhs.name
        FLAT.std[FLAT.idx] <- is_std
        rhs.mod <- 0L

        FLAT.rhs.mod.idx[FLAT.idx] <- rhs.mod
        if (rhs.mod > 0L) {
          MOD.idx <- MOD.idx + 1L
        }
      }
    }
  }

  mod.idx <- which(FLAT.rhs.mod.idx > 0L)
  FLAT.rhs.mod.idx[mod.idx] <- 1:length(mod.idx)
  
  FLAT <- data.frame(lhs = FLAT.lhs,
                     op = FLAT.op,
                     rhs = FLAT.rhs,
                     std = FLAT.std,
                     mod.idx = FLAT.rhs.mod.idx,
                     stringsAsFactors = FALSE
                     )

  sscore_df <- FLAT[FLAT$op == "=", ]
  if (nrow(sscore_df) > 0) sscore_df$type  <- NA

  name_ss <- vapply(
    split(sscore_df, sscore_df$lhs),
    function(x) unique(x$std),
    FUN.VALUE = logical(1)
  )

  cmv_df <- FLAT[FLAT$op == "~", ]

  # If any variable has a standardized sscore, assign it name
  # otherwise a sscore, otherwise just observed
  cmv_df$type <-
    with(cmv_df,
         ifelse(rhs %in% names(name_ss[name_ss]), "ss_std",
                ifelse(rhs %in% names(name_ss[!name_ss]), "ss", "observed")
                )
         )

  final_df <- rbind(sscore_df, cmv_df)
  final_df$std <- NULL

  if (as_list) {
    final_df <- lapply(final_df, identity)
  }

  final_df
}

syntax_parse_rhs <- function (rhs) {
  out <- list()

  all_terms <- attr(terms(rhs), "term.labels")
  out <- setNames(vector("list", length(all_terms)), all_terms)

  if (length(out) > 1L) {
    rhs.names <- names(out)
    while (!is.na(idx <- which(duplicated(rhs.names))[1L])) {
      dup.name <- rhs.names[idx]
      orig.idx <- match(dup.name, rhs.names)
      merged <- c(out[[orig.idx]], out[[idx]])
      if (!is.null(merged)) 
        out[[orig.idx]] <- merged
      out <- out[-idx]
      rhs.names <- names(out)
    }
  }
  
  out
}

library(essurvey)
selected_vars <- c("ppltrst",
                   "trstprl",
                   "stflife",
                   "stfeco")

ess_model <-
  "# First cmv
   ~ stflife + stfeco
   # Second cmv
   ~ ppltrst + trstprl
   # Third cmv
   ~ stflife + ppltrst"

# Download the ESS data and clear missing values
ess4es_complete <- import_country("Spain", 4)[selected_vars]

parsed_model <-  me_parse_model(ess_model)

me_data <- data.frame(stringsAsFactors=FALSE,
                      question = c("ppltrst", "trstprl", "stflife", "stfeco"),
                      reliability = c(0.729, 0.815, 0.655, 0.823),
                      validity = c(0.951, 0.944, 0.94, 0.903),
                      quality = c(0.693, 0.77, 0.615, 0.743)
                      )

cmv_df <- parsed_model[parsed_model$op == "~", ]
selected_vars <- unique(cmv_df[, "rhs"])
me_data <- me_data[match(selected_vars, me_data$question), ]

qual_corr <- me_correlate(
  ess4es_complete[, selected_vars],
  diag_adj = me_data$quality
)


cmv_groups <- split(cmv_df, cmv_df$lhs)
list_cmv_vars <- lapply(cmv_groups, `[[`, "rhs")
res <- me_cmv_cor_(qual_corr, me_data, list_cmv_vars[[1]])

for (vars in list_cmv_vars[-1]) {
  print(vars)
  res <- me_cmv_cor_(res, me_data, vars)
}

res


# You left off here
# You already have a parser that works, and you have a working
# example of correctiong for several observed variables for
# their CMV.

# Next step is to generalize to when you have sumscores. Sumscores
# need to happen before any of this and the quality
# of the sumscore needs to be added to the me_data before doing
# any correlation. So start with a parser that extracts only sumscores
# both standardized and normal, create this columns in the supplied
# data and if any of the sumscores are used in the CMV, you need
# to add it's quality in the me_data before doing any correlation.
# After that, you can just calculate the CMV of an observed and a
# composite score.

# Once you finish that, you were thinking about how to
# dispatch the specific operation of each CMV unit. For example,
# if you have a CMV between an observed and sscore, how
# would you translate that to the function that processes this
# specific case? You were thinking of doing something
# like a switch statemente to pass the desired function.
# This is possible now that you have standard evaluation
# functions for me_cmv_cor, me_cmv_cov and me_sscore.
# This is nice because since the user does not have
# room to add any arguments, so the function brought
# from switch doesn't need to have any partial arguments.


# When you got sscore + cmv working, you need to add the checks
# of whether the sscore variables are indeed in the data.
# This, I think, can be done automatically be model.matrix
# if you pass the sumscores, taking out one manual check.

# Then all variables in CMV must either in the data or the sscore definition
# this you need to give an explicit check and raise an error.

# I was thinking that `medesign` should accept the data, the me_data
# and the model definition and it should only create the sscore variables
# check that the cmv variables are in the data/me_data, create any quality
# of the composite sscore. I think I don't have to extract the part that checks
# which values are missing for the me_data because it's already done inside
# the functions. However, look at it and if possible, move it to me design
# so that all checks are done there.


# Tests
# when parsed string has the same variable used twice in CMV or sumscore,
# everything is ok
```
